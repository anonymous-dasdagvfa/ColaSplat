# -*- coding:utf-8 -*-
# 
# Author: 
# Time: 

import torch
import fnmatch
import numpy as np
import os
from scene_admm import GaussianModel
#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
#  äº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰ ä¼˜åŒ–ç®—æ³•ï¼Œé€šå¸¸ç”¨äºè§£å†³å¸¦æœ‰çº¦æŸå’Œæ­£åˆ™åŒ–çš„ä¼˜åŒ–é—®é¢˜ã€‚
class ADMM:
    def __init__(self, gsmodel: GaussianModel, rho1, rho2, device):
        self.gsmodel = gsmodel 
        self.device = device # æŒ‡å®šå¼ é‡å­˜å‚¨çš„ä½ç½®ï¼ˆCPU æˆ– GPUï¼‰
        self.init_rho1 = rho1 # æ ‡é‡ï¼Œç”¨äº ADMM ä¸­çš„æƒ©ç½šé¡¹
        self.init_rho2 = rho2 #

        # uå’Œzæ˜¯ADMMç®—æ³•ä¸­çš„è¾…åŠ©å˜é‡ï¼ˆå¯¹å¶å˜é‡ï¼‰
        self.u1 = {}
        self.u2 = {}
        self.z2 = {}
        self.z1 = {}

        self.rho1 = self.init_rho1
        self.rho2 = self.init_rho2

        opacity = self.gsmodel.get_opacity
        sh = self.gsmodel._features_rest

        self.u1 = torch.zeros(opacity.shape).to(device)
        self.z1 = torch.Tensor(opacity.data.cpu().clone().detach()).to(device)
        self.u2 = torch.zeros(sh.shape).to(device)
        self.z2 = torch.Tensor(sh.data.cpu().clone().detach()).to(device)

    # ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­ u é€æ­¥é€¼è¿‘ x å’Œ z çš„ä¸€è‡´æ€§
    def update1(self, threshold, update_u = True):
        # 1. è®¡ç®— z çš„è¾“å…¥ï¼Œç­‰ä»·äºåŸå§‹å˜é‡ + æ‹‰æ ¼æœ—æ—¥ä¹˜å­ Î»
        z = self.gsmodel.get_opacity + self.u1

        # 2. æ›´æ–° zï¼Œä½¿å…¶æ»¡è¶³çº¦æŸ (è¿™é‡Œæ˜¯è£å‰ª/ç¨€ç–åŒ–ç­‰æ“ä½œ)  
        self.z1 = torch.Tensor(self.prune_z(z, threshold)).to(self.device)
        
        # 3. æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜å­ Î»ï¼ˆu1ï¼‰ é€æ­¥æƒ©ç½šåŸå§‹å˜é‡å’Œè¾…åŠ©å˜é‡çš„å·®è·ï¼Œä¿ƒè¿›ä¸¤è€…æ”¶æ•›ä¸€è‡´
        if update_u: 
            with torch.no_grad():
                diff =  self.gsmodel.get_opacity - self.z1
                self.u1 += diff

    def update2(self, kmeans_sh_q, update_u=True, assign = True, update_center = True):

        if kmeans_sh_q.vec_dim == 0:
            kmeans_sh_q.vec_dim = self.gsmodel._features_rest.shape[1] * self.gsmodel._features_rest.shape[2]


        # è®¡ç®—x + u
        z_input = self.gsmodel._features_rest + self.u2
        feat = z_input.reshape(-1, kmeans_sh_q.vec_dim)  # å½¢çŠ¶ (N, D)

        # Step 1: å¯¹æ¯ä¸ªæ ·æœ¬ç‚¹æ‰¾åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒç´¢å¼•ï¼ˆèšç±»åˆ†é…ï¼‰
        kmeans_sh_q.update_centers(self.gsmodel._features_rest)
        kmeans_sh_q.cluster_assign(feat)  # è¿™é‡Œæ›´æ–° kmeans_sh_q.nn_index
        
        # Step 2: æ ¹æ®ç´¢å¼•å¾—åˆ°é‡åŒ–åçš„ä¸­å¿ƒå‘é‡ï¼ˆæŠ•å½±æ“ä½œï¼‰
        indices = kmeans_sh_q.nn_index.long()
        centers = kmeans_sh_q.centers  # ç æœ¬ä¸­å¿ƒ
        quantized_values = centers[indices]  # (N, vec_dim)

        # Step 3: é‡æ–° reshape å›å»ï¼Œæ›´æ–°è¾…åŠ©å˜é‡ z
        self.z2 = quantized_values.reshape(z_input.shape).to(self.device)

        # Step 4: æ›´æ–°å¯¹å¶å˜é‡ u
        if update_u:
            with torch.no_grad():
                diff = self.gsmodel._features_rest - self.z2
                self.u2 += diff



    #  è¯¥æ–¹æ³•æ ¹æ®ä¸åŒçš„ç­–ç•¥ï¼ˆç”± opt å‚æ•°æ§åˆ¶ï¼‰æ¥æ›´æ–° zï¼š
    def prune_z(self, z, threshold):
        z_update = self.metrics_sort(z, threshold)  
        return z_update
    

    # ä½¿opacityå’Œz1ä¹‹é—´çš„å·®å¼‚æœ€å°åŒ– 
    def get_admm_loss_1(self): 
        return 0.5 * self.rho1 * (torch.norm(self.gsmodel.get_opacity - self.z1 + self.u1, p=2)) ** 2
    
    # ä½¿features_restå’Œz2ä¹‹é—´çš„å·®å¼‚æœ€å°åŒ–
    def get_admm_loss_2(self): 
        return 0.5 * self.rho2 * (torch.norm(self.gsmodel._features_rest - self.z2 + self.u2, p=2)) ** 2

    def adjust_rho(self, epoch, epochs, factor=5): # æ ¹æ®è®­ç»ƒçš„å½“å‰è¿›åº¦ï¼ˆepoch å’Œ epochsï¼‰è°ƒæ•´ rho å€¼ã€‚é€šå¸¸ï¼Œrho ä¼šéšç€è®­ç»ƒçš„è¿›å±•è€Œå¢å¤§ï¼Œä»è€Œå¢åŠ å¯¹çº¦æŸçš„æƒ©ç½š
        if epoch > int(0.85 * epochs):
            self.rho1 = factor * self.init_rho1
    
    def metrics_sort(self, z, threshold): # æ ¹æ®é€æ˜åº¦çš„æ’åºå€¼æ¥æ›´æ–° zã€‚å®ƒé€šè¿‡å°†é€æ˜åº¦æŒ‰å‡åºæ’åºå¹¶åº”ç”¨ä¸€ä¸ªé˜ˆå€¼æ¥é€‰æ‹©é€æ˜åº¦å€¼
        index = int(threshold * len(z))
        z_sort = {}
        z_update = torch.zeros(z.shape)
        z_sort, _ = torch.sort(z, 0)
        z_threshold = z_sort[index-1]
        z_update= ((z > z_threshold) * z)  
        return z_update
    
    def metrics_sample(self, z, opt): # è¯¥æ–¹æ³•æ ¹æ®é€æ˜åº¦å€¼çš„ç›¸å¯¹æƒé‡è¿›è¡Œéšæœºé‡‡æ ·ã€‚é¦–å…ˆï¼Œå°†é€æ˜åº¦å€¼å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶æŒ‰æ­¤åˆ†å¸ƒéšæœºé€‰æ‹©æ ·æœ¬ã€‚
        index = int((1 - opt.pruning_threshold) * len(z))
        prob = z / torch.sum(z)
        prob = prob.reshape(-1).cpu().numpy()
        indices = torch.tensor(np.random.choice(len(z), index, p = prob, replace=False))
        expand_indices = torch.zeros(z.shape[0] - len(indices)).int()
        indices = torch.cat((indices, expand_indices),0).to(self.device)
        z_update = torch.zeros(z.shape).to(self.device)
        z_update[indices] = z[indices]
        return z_update

    def metrics_imp_score(self, z, imp_score, opt): # è¯¥æ–¹æ³•åŸºäºé‡è¦æ€§åˆ†æ•°ï¼ˆimp_scoreï¼‰æ›´æ–° zã€‚é‡è¦æ€§åˆ†æ•°ä½äºæŸä¸ªé˜ˆå€¼çš„é€æ˜åº¦å€¼ä¼šè¢«ç½®ä¸º 0ï¼Œä»è€Œå¯¹é‡è¦æ€§è¾ƒä½çš„éƒ¨åˆ†è¿›è¡Œç¨€ç–åŒ–ã€‚
        index = int(opt.pruning_threshold * len(z))
        imp_score_sort = {}
        imp_score_sort, _ = torch.sort(imp_score, 0)
        imp_score_threshold = imp_score_sort[index-1]
        indices = imp_score < imp_score_threshold 
        z[indices == 1] = 0  
        return z        



def get_unactivate_opacity(gaussians):
    opacity = gaussians._opacity[:, 0]
    scores = opacity
    return scores

def get_pruning_mask(scores, threshold):        
    scores_sorted, _ = torch.sort(scores, 0)
    threshold_idx = int(threshold * len(scores_sorted))
    abs_threshold = scores_sorted[threshold_idx - 1]
    mask = (scores <= abs_threshold).squeeze()
    return mask

def check_grad_leakage(model, optimizer=None):
    """
    é€‚ç”¨äºé nn.Module çš„æ¨¡å‹ï¼Œè‡ªè¡Œéå†å±æ€§æ£€æŸ¥å‚æ•°æ¢¯åº¦çŠ¶æ€ã€‚
    """
    print("\n=== ğŸš¨ æ£€æŸ¥æ¢¯åº¦æ³„éœ²å’Œä¼˜åŒ–å™¨å‚æ•°ï¼ˆè‡ªå®šä¹‰æ¨¡å‹ï¼‰ ===")
    leak_found = False

    for name in dir(model):
        param = getattr(model, name)
        if isinstance(param, torch.Tensor) and param.requires_grad is not None:
            if param.grad is not None and not param.requires_grad:
                print(f"âš ï¸ å‚æ•° '{name}' æœ‰ gradï¼Œä½† requires_grad=Falseï¼å¯èƒ½æ³„éœ²ã€‚")
                leak_found = True
            elif param.requires_grad:
                print(f"âœ… å‚æ•° '{name}' è®¾ç½®ä¸ºå¯è®­ç»ƒ (requires_grad=True)ã€‚")
            else:
                print(f"ğŸ”’ å‚æ•° '{name}' ä¸å¯è®­ç»ƒ (requires_grad=False)ã€‚")

    # æ£€æŸ¥ optimizer å‚æ•°ç»„
    if optimizer is not None:
        print("\n=== ğŸ” æ£€æŸ¥ Optimizer ä¸­çš„å‚æ•° ===")
        for i, group in enumerate(optimizer.param_groups):
            for p in group['params']:
                if not p.requires_grad:
                    print(f"âš ï¸ Optimizer param_group[{i}] ä¸­åŒ…å« requires_grad=False çš„å‚æ•°")
                    leak_found = True

    if not leak_found:
        print("âœ… æ²¡å‘ç°æ³„éœ²æˆ–å¼‚å¸¸å‚æ•°ã€‚")
    raise("=== æ£€æŸ¥å®Œæˆ ===\n")
